{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Corpus Analysis with Cosine Similarity\n",
    "\n",
    "Author: Lucas van der Deijl, University of Amsterdam <br/>\n",
    "Version: 9 December 2020 <br/>\n",
    "Contact: l.a.vanderdeijl@uva.nl, www.lucasvanderdeijl.nl <br/>\n",
    "Project: 'Radical Rumours' (Funded by NWO 2017-2021) <br/>\n",
    "\n",
    "## Aim of this program\n",
    "\n",
    "The program offers a basic method to analyse textual similarity between document pairs from either one corpus or two different corpora. Textual similarity is formalised as [cosine similarity](https://www.sciencedirect.com/topics/computer-science/cosine-similarity) based on [tf-idf values](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for every word type in the corpus. The results are visualised as a heatmap and can be stored as a csv-file.\n",
    "\n",
    "This Jupyter notebook can be used to reuse the code or to replicate the analysis with different corpora. Run each code block individually or use the 'Run all'-option from the Cell-tab.\n",
    "\n",
    "## Pipeline\n",
    "The pipeline desigend to achieve the program's aim performs the following steps:\n",
    "\n",
    "+ import the required modules\n",
    "+ install missing libraries (if needed)\n",
    "+ define functions for preprocessing and parsing\n",
    "+ define the filepath to the location of your corpus\n",
    "+ load and preprocess your corpus\n",
    "+ create a term-document matrix with tfidf values\n",
    "+ creae a document-document matrix with cosine similarity values\n",
    "+ visualise the document matrix as a heatmap\n",
    "+ store the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries\n",
    "\n",
    "First, the required libaries and resources need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # We're going to use the Python package scikit-learn to transform texts into vectors of TF-IDF values\n",
    "from scipy import spatial\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install missing libraries\n",
    "\n",
    "In case you got an error after the previous step because not all of the required modules are installed, you can uncomment (remove the '#') the relevant install-command below and run the code. Once the module is installed, run the block above again to import it before moving on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for preprocessing and parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "  stopwords = open((\"Resources/stopwoorden.txt\"), 'rt', encoding='utf-8').read().split()\n",
    "  punct = punctuation\n",
    "  tokens = word_tokenize(doc)\n",
    "  lowercase_tokens = [token.lower() for token in tokens]\n",
    "  punct_and_stops_removed = \" \".join([token for token in lowercase_tokens if (token not in stopwords) and (token not in punct)]) \n",
    "  preprocessed_doc = punct_and_stops_removed\n",
    "  return(preprocessed_doc)\n",
    "\n",
    "def parse_corpus(corpus_location):\n",
    "    corpus = []\n",
    "    titles = []\n",
    "    for filename in os.listdir(corpus_location):\n",
    "        title = filename.split(\"_\")[2]\n",
    "        titles.append(title)\n",
    "        file = open((corpus_location + filename), 'rt', encoding='utf-8')\n",
    "        preprocessed_text = preprocess(file.read())\n",
    "        corpus.append(preprocessed_text)\n",
    "        file.close()\n",
    "    return(titles, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the filepath to the location of your corpus\n",
    "\n",
    "Sample data is included in the form of 17 seventeenth-century Dutch translations of books by Descartes and Spinoza. These texts have been prepared for analysis through partial spelling normalisation and lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths to corpora\n",
    "path_to_corpusfolder = \"Corpus/\"\n",
    "path_to_corpus_Descartes = path_to_corpusfolder + \"Descartes/\"\n",
    "path_to_corpus_Spinoza = path_to_corpusfolder + \"Spinoza/\"\n",
    "#os.listdir(path_to_corpusfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_corpus = parse_corpus(path_to_corpus_Spinoza)\n",
    "target_corpus = parse_corpus(path_to_corpus_Spinoza)\n",
    "\n",
    "document_titles = source_corpus[0] + target_corpus[0]\n",
    "total_corpus = source_corpus[1] + target_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a term-document matrix with tfidf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=0) # set parameters for vectorization\n",
    "term_doc_matrix = vect.fit_transform(total_corpus)\n",
    "term_doc_matrix_array = term_doc_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a document-document matrix with cosine similarity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = {}\n",
    "\n",
    "for row_counter, tfidf_array_source in enumerate(term_doc_matrix_array[:len(source_corpus[1])]):\n",
    "    source_index = row_counter\n",
    "    source_title = document_titles[source_index]\n",
    "    similarity_matrix[source_title] = {}\n",
    "    for column_counter, tfidf_array_target in enumerate(term_doc_matrix_array[len(source_corpus[1]):]): \n",
    "        target_index = len(source_corpus[1]) + column_counter\n",
    "        target_title = document_titles[target_index]\n",
    "        cos_similarity = 1- spatial.distance.cosine(tfidf_array_source, tfidf_array_target) # substracted from 1 to create similarity metric\n",
    "        similarity_matrix[source_title][target_title] = cos_similarity\n",
    "\n",
    "heatmapdata = []\n",
    "for source_key in similarity_matrix:\n",
    "    values_list = [value for value in similarity_matrix[source_key].values()]\n",
    "    heatmapdata.append(values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the document matrix as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"font.family\"] = \"Garamond\"\n",
    "\n",
    "source_titles = source_corpus[0]\n",
    "target_titles = target_corpus[0] \n",
    "\n",
    "data = heatmapdata\n",
    "\n",
    "mask = np.zeros_like(data)\n",
    "mask[np.triu_indices_from(mask)] = False # Switch to 'True' if source corpus = target corpus, to exclude redundant data\n",
    "\n",
    "ax = sns.heatmap(data, \n",
    "                 mask=mask, \n",
    "                 cmap=\"YlGnBu\", \n",
    "                 annot=True, \n",
    "                 xticklabels=target_titles, \n",
    "                 yticklabels=source_titles, \n",
    "                 cbar=True, \n",
    "                 vmin=0, \n",
    "                 vmax=1)\n",
    "\n",
    "#plt.savefig(\"Output/IMAGE_NAME.png\", bbox_inches='tight', dpi=300) # Uncomment to save the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = open(\"Output/output.csv\", 'w', encoding=\"UTF-8\")\n",
    "heading = \";\" + \";\".join(source_corpus[0]) +\"\\n\"\n",
    "outputfile.write(heading)\n",
    "\n",
    "for counter, row in enumerate(heatmapdata):\n",
    "    values = \"\"\n",
    "    for value in row:\n",
    "        values += str(value) + \";\"   \n",
    "    \n",
    "    output_row = str(source_corpus[0][counter]) + \";\" + values +\"\\n\"\n",
    "    outputfile.write(output_row)\n",
    "outputfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
